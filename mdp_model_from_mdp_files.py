#!/usr/bin/env python3
"""
mdp_model_from_mdp_files.py

Loads the 4 files generated by build_mdp_from_cleaned.py:
 - mdp_events.csv
 - mdp_states.csv
 - mdp_transitions.json
 - mdp_setup.json

Produces:
 - Transition tensor P[s,a_k,a_g,s']
 - Reward tensor R[s,a_k,a_g]
 - Empirical policies (kicker, keeper)
 - Single-agent MDP solve (kicker vs empirical keeper)
 - Zero-sum Markov game solve (value iteration with per-state saddle-point LP)

Outputs saved:
 - mdp_values_singleagent.json
 - mdp_policy_singleagent.json
 - mdp_values_zerosum.json
 - mdp_policy_zerosum.json

Requires:
  pip install pandas numpy scipy
"""
import json
import numpy as np
import pandas as pd
from collections import defaultdict
from scipy.optimize import linprog
import os
import argparse

# ---------------------
# Utilities
# ---------------------
def load_files(prefix='mdp_'):
    events_file = prefix + 'events.csv'
    states_file = prefix + 'states.csv'
    trans_file  = prefix + 'transitions.json'
    setup_file  = prefix + 'setup.json'

    if not os.path.exists(events_file):
        raise FileNotFoundError(events_file + " not found.")
    if not os.path.exists(states_file):
        raise FileNotFoundError(states_file + " not found.")
    if not os.path.exists(trans_file):
        raise FileNotFoundError(trans_file + " not found.")
    if not os.path.exists(setup_file):
        raise FileNotFoundError(setup_file + " not found.")

    events = pd.read_csv(events_file)
    states = pd.read_csv(states_file)['state'].tolist()
    with open(trans_file, 'r') as f:
        trans = json.load(f)
    with open(setup_file, 'r') as f:
        setup = json.load(f)

    return events, states, trans, setup

def make_index_map(items):
    return {item: idx for idx, item in enumerate(items)}

def parse_trans_key(key_str):
    # key_str was saved as str((state, action_kicker, action_keeper))
    # Example: "('RRR_RRR','L','R')"
    # We'll eval safely by stripping outer parentheses and splitting
    try:
        tup = eval(key_str)
        return tup  # (state, a_k, a_g)
    except Exception:
        # fallback: try manual parsing
        s = key_str.strip()
        if s.startswith('(') and s.endswith(')'):
            s = s[1:-1]
        parts = s.split(',', 2)
        parts = [p.strip().strip("'\"") for p in parts]
        if len(parts) == 3:
            return (parts[0], parts[1], parts[2])
        else:
            return None

# ---------------------
# Build tensors from files
# ---------------------
def build_tensors(events, states, trans, setup):
    """
    Build:
      - P: transition tensor shape (S, Ak, Ag, S)
      - R: reward tensor shape (S, Ak, Ag)
    Also compute empirical policies:
      - pi_k_emp[s,a_k]  (kicker probability)
      - pi_g_emp[s,a_g]  (keeper probability)
    """
    state_to_idx = make_index_map(states)
    # Action lists from setup (fall back to L/R)
    actions_k = setup.get('actions', {}).get('kicker', ['L','R'])
    actions_g = setup.get('actions', {}).get('keeper', ['L','R'])
    ak_to_idx = make_index_map(actions_k)
    ag_to_idx = make_index_map(actions_g)

    S = len(states)
    Ak = len(actions_k)
    Ag = len(actions_g)

    # Initialize P and R
    P = np.zeros((S, Ak, Ag, S), dtype=float)
    R = np.zeros((S, Ak, Ag), dtype=float)
    counts_R = np.zeros((S, Ak, Ag), dtype=int)  # count events for averaging reward

    # Fill P using trans (which maps stringified (s,ak,ag) -> {s':prob})
    # trans keys are stringified tuples
    for key_str, dests in trans.items():
        parsed = parse_trans_key(key_str)
        if parsed is None:
            continue
        s_str, ak_str, ag_str = parsed
        if s_str not in state_to_idx:
            continue
        s = state_to_idx[s_str]
        if ak_str not in ak_to_idx or ag_str not in ag_to_idx:
            # skip unknown actions
            continue
        ak = ak_to_idx[ak_str]
        ag = ag_to_idx[ag_str]
        for s_next, p in dests.items():
            if s_next not in state_to_idx:
                continue
            sn = state_to_idx[s_next]
            P[s, ak, ag, sn] = float(p)

    # If some entries of P rows are all zero (missing), we'll fall back to empirical counts from events
    # Next, build reward from events dataframe if there's a reward column, else synthesize as (kicker != keeper)
    events_cols = events.columns.tolist()
    has_reward = 'reward' in events_cols
    # Build counts and reward sums from events (prefer real reward)
    for _, row in events.iterrows():
        s = row['state']
        if pd.isna(s):
            continue
        if s not in state_to_idx:
            continue
        si = state_to_idx[s]
        ak = row['action_kicker']
        ag = row['action_keeper']
        if pd.isna(ak) or pd.isna(ag):
            continue
        if ak not in ak_to_idx or ag not in ag_to_idx:
            continue
        aki = ak_to_idx[ak]
        agi = ag_to_idx[ag]
        # reward
        if has_reward:
            # interpret 1/0
            rv = row['reward']
            try:
                rv = float(rv)
            except Exception:
                rv = 1.0 if str(rv).lower() in ['goal','g','1','true','t','yes','y'] else 0.0
        else:
            # synthetic rule: if same direction -> save (0), else score (1)
            rv = 1.0 if ak != ag else 0.0
        R[si, aki, agi] += rv
        counts_R[si, aki, agi] += 1

        # If trans entry is missing for this (s,ak,ag), we can increment empirical counts for P
        next_s = row.get('next_state', None)
        if pd.notna(next_s) and next_s in state_to_idx:
            nxt = state_to_idx[next_s]
            # We'll accumulate empirical counts in a temp dict and normalize later for entries where P is zero.
            # For now store as additional attribute (we will handle below)
            # To avoid extra structure, we increment P_counts in a separate dict
            pass

    # Finalize R by averaging where counts > 0
    mask = counts_R > 0
    R[mask] = R[mask] / counts_R[mask]
    # For (s,ak,ag) with zero counts, R remains 0 by default - this is acceptable but should be documented

    # Check P rows: if any (s,ak,ag) row sums to zero, try to estimate from empirical events:
    # Build empirical next-state counts from events
    empirical_next = defaultdict(lambda: defaultdict(int))  # key: (s,ak,ag) -> dict next->count
    for _, row in events.iterrows():
        s = row['state']
        if pd.isna(s): continue
        if s not in state_to_idx: continue
        si = state_to_idx[s]
        ak = row['action_kicker']
        ag = row['action_keeper']
        if pd.isna(ak) or pd.isna(ag): continue
        if ak not in ak_to_idx or ag not in ag_to_idx: continue
        aki = ak_to_idx[ak]
        agi = ag_to_idx[ag]
        nxt = row.get('next_state', None)
        if pd.isna(nxt) or nxt not in state_to_idx: continue
        nxi = state_to_idx[nxt]
        empirical_next[(si,aki,agi)][nxi] += 1

    # normalize empirical_next into P for missing rows
    for (si,aki,agi), dests in empirical_next.items():
        if np.isclose(P[si,aki,agi].sum(), 0.0):
            total = sum(dests.values())
            if total > 0:
                for sn, cnt in dests.items():
                    P[si,aki,agi,sn] = cnt / total

    # Final safety: ensure rows sum to 1 (or zero if no info)
    for si in range(P.shape[0]):
        for aki in range(P.shape[1]):
            for agi in range(P.shape[2]):
                ssum = P[si,aki,agi].sum()
                if ssum > 0:
                    P[si,aki,agi] /= ssum  # normalize to exact 1
                else:
                    # leave as zero vector -> no transitions observed for this triple
                    pass

    # Empirical policies: count from events
    pi_k_counts = np.zeros((S, Ak), dtype=float)
    pi_g_counts = np.zeros((S, Ag), dtype=float)
    for _, row in events.iterrows():
        s = row['state']
        if pd.isna(s): continue
        if s not in state_to_idx: continue
        si = state_to_idx[s]
        ak = row['action_kicker']
        ag = row['action_keeper']
        if pd.isna(ak) or pd.isna(ag): continue
        if ak in ak_to_idx:
            pi_k_counts[si, ak_to_idx[ak]] += 1
        if ag in ag_to_idx:
            pi_g_counts[si, ag_to_idx[ag]] += 1
    # normalize
    pi_k_emp = np.zeros_like(pi_k_counts)
    pi_g_emp = np.zeros_like(pi_g_counts)
    with np.errstate(divide='ignore', invalid='ignore'):
        row_sum_k = pi_k_counts.sum(axis=1)
        mask_k = row_sum_k > 0
        pi_k_emp[mask_k] = pi_k_counts[mask_k] / row_sum_k[mask_k][:,None]
        row_sum_g = pi_g_counts.sum(axis=1)
        mask_g = row_sum_g > 0
        pi_g_emp[mask_g] = pi_g_counts[mask_g] / row_sum_g[mask_g][:,None]

    meta = {
        'states': states,
        'state_to_idx': state_to_idx,
        'actions_k': actions_k,
        'actions_g': actions_g
    }

    return P, R, pi_k_emp, pi_g_emp, meta

# ---------------------
# Single-agent MDP (kicker) given keeper empirical policy
# ---------------------
def build_single_agent_mdp_from_empirical(P, R, pi_g_emp, gamma=0.95):
    """
    Marginalize keeper actions using empirical keeper policy pi_g_emp(s, a_g)
    This produces:
      - Pk[s, a_k, s'] = sum_{a_g} pi_g_emp[s,a_g] * P[s,a_k,a_g,s']
      - Rk[s, a_k] = sum_{a_g} pi_g_emp[s,a_g] * R[s,a_k,a_g]
    Then run standard value iteration for the kicker as a single-agent MDP.
    """
    S, Ak, Ag, _ = P.shape
    Pk = np.zeros((S, Ak, S), dtype=float)
    Rk = np.zeros((S, Ak), dtype=float)
    for s in range(S):
        for ak in range(Ak):
            for ag in range(Ag):
                prob_g = pi_g_emp[s, ag]
                if prob_g > 0:
                    Pk[s, ak] += prob_g * P[s, ak, ag]
                    Rk[s, ak] += prob_g * R[s, ak, ag]

    # Value iteration
    V = np.zeros(S, dtype=float)
    policy = np.zeros((S, Ak), dtype=float)
    tol = 1e-6
    max_iter = 5000
    for it in range(max_iter):
        V_new = np.zeros_like(V)
        for s in range(S):
            # Q(s,a) = Rk[s,a] + gamma * sum_s' Pk[s,a,s'] * V[s']
            Qs = Rk[s] + gamma * (Pk[s] @ V)
            V_new[s] = Qs.max()  # kicker chooses action to maximize expected return
            # store greedy policy
            a_best = np.argmax(Qs)
            policy[s] = 0
            policy[s, a_best] = 1
        if np.max(np.abs(V_new - V)) < tol:
            V = V_new
            break
        V = V_new
    return V, policy, Pk, Rk

# ---------------------
# Zero-sum Markov game solve (value iteration with per-state saddle LP)
# ---------------------
def solve_zerosum_markov_game(P, R, gamma=0.95, tol=1e-6, max_iter=5000):
    """
    Solve zero-sum stochastic game (kicker maximizes, keeper minimizes).
    At each iteration, for each state s compute the matrix game:
      M[a_k, a_g] = R[s,a_k,a_g] + gamma * sum_{s'} P[s,a_k,a_g,s'] * V[s']
    Then compute v_s = value of the zero-sum matrix game (max_min).
    Use LP to compute value and kicker policy p (mixed).
    Repeat until V converges.

    Returns:
       V (values), pi_k (S x Ak), pi_g (S x Ag)
    """
    S, Ak, Ag, _ = P.shape
    V = np.zeros(S, dtype=float)

    # helper: solve zero-sum matrix game via LP for current M matrix
    def solve_matrix_game_lp(M):
        # M is Ak x Ag, kicker wants max_min p^T M q
        # Solve for p and v: maximize v subject to p >=0, sum p =1, and for all j: sum_i p_i * M[i,j] >= v
        # Convert to LP minimize -v subject to constraints:
        # Variables: [p_0..p_{Ak-1}, v]  length Ak+1
        c = np.zeros(Ak + 1)
        c[-1] = -1.0  # minimize -v --> maximize v
        # Constraints: for each column j: sum_i p_i * M[i,j] - v >= 0  =>  -sum_i p_i * M[i,j] + v <= 0 (for linprog form)
        A_ub = []
        b_ub = []
        for j in range(Ag):
            row = np.zeros(Ak + 1)
            row[:Ak] = -M[:, j]  # -sum_i p_i * M[i,j]
            row[-1] = 1.0        # + v
            A_ub.append(row)
            b_ub.append(0.0)
        # equality: sum p_i = 1
        A_eq = np.zeros((1, Ak + 1))
        A_eq[0, :Ak] = 1.0
        A_eq[0, -1] = 0.0
        b_eq = np.array([1.0])
        # bounds: p_i >= 0, v free (but we can bound v)
        bounds = [(0.0, 1.0)] * Ak + [(None, None)]  # v unrestricted
        # initial guess not supported in linprog; use high precision method
        res = linprog(c, A_ub=np.array(A_ub), b_ub=np.array(b_ub),
                      A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if not res.success:
            # fallback: return uniform policy and value = min_j max_i M[i,j]
            p = np.ones(Ak) / Ak
            v = np.min(np.max(M, axis=0))
            return v, p
        sol = res.x
        p = sol[:Ak]
        v = sol[-1]
        # numerical cleanup
        p[p < 1e-12] = 0.0
        if p.sum() > 0:
            p = p / p.sum()
        return v, p

    # main iteration
    for it in range(max_iter):
        V_new = np.zeros_like(V)
        pi_k = np.zeros((S, Ak))
        # For each state, form M and solve matrix game
        for s in range(S):
            M = np.zeros((Ak, Ag))
            for aki in range(Ak):
                for agi in range(Ag):
                    # immediate reward + discounted expected V
                    cont = 0.0
                    # P[s,aki,agi,:] is vector over s'
                    cont = (P[s, aki, agi] @ V)
                    M[aki, agi] = R[s, aki, agi] + gamma * cont
            v_s, p_s = solve_matrix_game_lp(M)
            V_new[s] = v_s
            pi_k[s] = p_s
        if np.max(np.abs(V_new - V)) < tol:
            V = V_new
            break
        V = V_new
    # After we have pi_k, compute pi_g best responses per state by solving column player minimization
    pi_g = np.zeros((S, Ag))
    for s in range(S):
        # Given pi_k[s], compute minimizer q to minimize p^T M q which reduces to choosing q minimizing expected value
        # For zero-sum 2-player finite matrix game, keeper optimal q can be computed by solving small LP:
        # minimize w subject to A_eq etc. But easiest: solve dual LP (we can solve same LP swapping roles)
        M = np.zeros((Ak, Ag))
        for aki in range(Ak):
            for agi in range(Ag):
                cont = (P[s, aki, agi] @ V)
                M[aki, agi] = R[s, aki, agi] + gamma * cont
        # Keeper minimizes max across rows; we can compute q by solving dual LP. Simpler: compute minimax q via solving primal LP for minimizer:
        # We'll solve min_{q} u subject to for all i: sum_j M[i,j] q_j <= u, sum q_j =1, q>=0
        # Variables: [q_0..q_{Ag-1}, u] minimize u
        c = np.zeros(Ag + 1)
        c[-1] = 1.0  # minimize u
        A_ub = []
        b_ub = []
        for i in range(Ak):
            row = np.zeros(Ag + 1)
            row[:Ag] = M[i, :]  # sum_j M[i,j] q_j <= u -> row * [q,u] <= u? need row - u <= 0
            row[-1] = -1.0
            A_ub.append(row)
            b_ub.append(0.0)
        A_eq = np.zeros((1, Ag + 1))
        A_eq[0, :Ag] = 1.0
        b_eq = np.array([1.0])
        bounds = [(0.0, 1.0)] * Ag + [(None, None)]
        res = linprog(c, A_ub=np.array(A_ub), b_ub=np.array(b_ub),
                      A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='highs')
        if not res.success:
            q = np.ones(Ag) / Ag
        else:
            q = res.x[:Ag]
            q[q < 1e-12] = 0.0
            if q.sum() > 0:
                q = q / q.sum()
        pi_g[s] = q
    return V, pi_k, pi_g

# ---------------------
# Main CLI
# ---------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--prefix", "-p", default="mdp_", help="Prefix for mdp files (default mdp_)")
    parser.add_argument("--gamma", "-g", default=0.95, type=float, help="Discount factor")
    parser.add_argument("--outdir", "-o", default=".", help="Output directory")
    args = parser.parse_args()

    events, states, trans, setup = load_files(prefix=args.prefix)
    P, R, pi_k_emp, pi_g_emp, meta = build_tensors(events, states, trans, setup)

    # Single-agent MDP (kicker vs empirical keeper)
    V_sa, pol_sa, Pk, Rk = build_single_agent_mdp_from_empirical(P, R, pi_g_emp, gamma=args.gamma)
    # Save single-agent outputs
    out_sa = {
        'V': V_sa.tolist(),
        'policy_greedy': pol_sa.tolist(),
        'states': states,
        'actions_k': meta['actions_k'],
        'actions_g': meta['actions_g']
    }
    with open(os.path.join(args.outdir, 'mdp_values_singleagent.json'), 'w') as f:
        json.dump(out_sa, f, indent=2)
    print("Saved single-agent MDP results to mdp_values_singleagent.json")

    # Zero-sum solve
    V_zs, pi_k_zs, pi_g_zs = solve_zerosum_markov_game(P, R, gamma=args.gamma)
    out_zs = {
        'V': V_zs.tolist(),
        'pi_k': pi_k_zs.tolist(),
        'pi_g': pi_g_zs.tolist(),
        'states': states,
        'actions_k': meta['actions_k'],
        'actions_g': meta['actions_g']
    }
    with open(os.path.join(args.outdir, 'mdp_values_zerosum.json'), 'w') as f:
        json.dump(out_zs, f, indent=2)
    print("Saved zero-sum MDP results to mdp_values_zerosum.json")

    # Print small diagnostics
    print("\nDiagnostics:")
    print("Num states:", len(states))
    print("Empirical pi_k (first 10 states):")
    for i in range(min(10, len(states))):
        print(states[i], "->", pi_k_emp[i].tolist(), " empirical counts normalized")
    print("Empirical pi_g (first 10 states):")
    for i in range(min(10, len(states))):
        print(states[i], "->", pi_g_emp[i].tolist())

if __name__ == "__main__":
    main()
